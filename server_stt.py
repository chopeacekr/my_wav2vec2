"""
Wav2Vec2 STT Server (CPU Optimized)
FastAPI ê¸°ë°˜ í•œêµ­ì–´ ìŒì„± ì¸ì‹ ì„œë²„

ëª¨ë¸: kresnik/wav2vec2-large-xlsr-korean
í¬íŠ¸: 8400
ìµœì í™”: CPU ì „ìš© (ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›)
"""

import io
import logging
import os
from typing import Dict

import librosa
import soundfile as sf
import torch
from fastapi import FastAPI, File, HTTPException, UploadFile
from fastapi.responses import JSONResponse
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

# ==========================================
# CPU ìµœì í™” ì„¤ì •
# ==========================================

# CPU ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
CPU_THREADS = int(os.getenv("OMP_NUM_THREADS", "4"))

# PyTorch CPU ìŠ¤ë ˆë“œ ì„¤ì •
torch.set_num_threads(CPU_THREADS)

# MKL ìŠ¤ë ˆë“œ ì„¤ì • (Intel CPU ìµœì í™”)
if os.getenv("MKL_NUM_THREADS") is None:
    os.environ["MKL_NUM_THREADS"] = str(CPU_THREADS)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="Wav2Vec2 STT Server (CPU)", version="1.0.0")

# ì „ì—­ ë³€ìˆ˜
processor = None
model = None
device = None

# ì§€ì› ì–¸ì–´ (í™•ì¥ ê°€ëŠ¥)
SUPPORTED_LANGUAGES = {
    "KR": {
        "model_id": "kresnik/wav2vec2-large-xlsr-korean",
        "name": "Korean",
        "sample_rate": 16000
    },
    # í–¥í›„ ë‹¤ë¥¸ ì–¸ì–´ ì¶”ê°€ ê°€ëŠ¥
    # "EN": {
    #     "model_id": "facebook/wav2vec2-base-960h",
    #     "name": "English",
    #     "sample_rate": 16000
    # }
}

DEFAULT_LANGUAGE = "KR"


@app.on_event("startup")
async def load_model():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ (CPU ìµœì í™”)"""
    global processor, model, device
    
    logger.info("ğŸš€ Wav2Vec2 STT Server ì‹œì‘ ì¤‘... (CPU Optimized)")
    
    # â­ CPU ì „ìš© ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cpu")
    logger.info(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    logger.info(f"ğŸ§µ CPU ìŠ¤ë ˆë“œ ìˆ˜: {CPU_THREADS}")
    
    # CPU ì •ë³´ ì¶œë ¥
    logger.info(f"ğŸ’» CPU ì½”ì–´ ìˆ˜: {os.cpu_count()}")
    logger.info(f"ğŸ”§ PyTorch ìŠ¤ë ˆë“œ: {torch.get_num_threads()}")
    
    # ê¸°ë³¸ í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ
    model_id = SUPPORTED_LANGUAGES[DEFAULT_LANGUAGE]["model_id"]
    logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {model_id}")
    logger.info(f"â³ ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (~1.2GB)")
    
    try:
        # Processor (í† í¬ë‚˜ì´ì €) ë¡œë“œ
        logger.info("ğŸ“¥ Processor ë‹¤ìš´ë¡œë“œ ì¤‘...")
        processor = Wav2Vec2Processor.from_pretrained(model_id)
        logger.info("âœ… Processor ë¡œë“œ ì™„ë£Œ")
        
        # â­ ëª¨ë¸ ë¡œë“œ (CPU ìµœì í™”)
        logger.info("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ìš©ëŸ‰ í¼, ì‹œê°„ ì†Œìš”)")
        model = Wav2Vec2ForCTC.from_pretrained(
            model_id,
            torch_dtype=torch.float32,  # â­ CPUëŠ” float32 ì‚¬ìš© (float16 ë¶ˆì•ˆì •)
            low_cpu_mem_usage=True,     # â­ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        )
        
        # CPUë¡œ ì´ë™ (ì´ë¯¸ CPUì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ)
        model.to(device)
        
        # â­ í‰ê°€ ëª¨ë“œ (ë“œë¡­ì•„ì›ƒ ë¹„í™œì„±í™”)
        model.eval()
        
        logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_info.rss / 1024 / 1024:.1f} MB")
        except ImportError:
            logger.info("ğŸ’¡ psutil ì„¤ì¹˜ ì‹œ ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸ ê°€ëŠ¥: pip install psutil")
        
        logger.info(f"ğŸ‰ Wav2Vec2 STT Server ì¤€ë¹„ ì™„ë£Œ! (í¬íŠ¸: 8400)")
        logger.info(f"ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥: 10ì´ˆ ì˜¤ë””ì˜¤ â†’ ì•½ {CPU_THREADS}ì½”ì–´ ê¸°ì¤€ 1.5-2ì´ˆ ì²˜ë¦¬")
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        logger.error(f"ğŸ’¡ í•´ê²° ë°©ë²•:")
        logger.error(f"   1. ì¸í„°ë„· ì—°ê²° í™•ì¸ (Hugging Face ë‹¤ìš´ë¡œë“œ í•„ìš”)")
        logger.error(f"   2. ìºì‹œ ì‚­ì œ: rm -rf ~/.cache/huggingface")
        logger.error(f"   3. ë©”ëª¨ë¦¬ í™•ì¸: ìµœì†Œ 4GB RAM ê¶Œì¥")
        raise


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "Wav2Vec2 STT Server (CPU Optimized)",
        "version": "1.0.0",
        "status": "running",
        "model": SUPPORTED_LANGUAGES[DEFAULT_LANGUAGE]["model_id"],
        "device": str(device),
        "cpu_threads": CPU_THREADS,
        "cpu_cores": os.cpu_count(),
        "supported_languages": list(SUPPORTED_LANGUAGES.keys())
    }


@app.get("/health")
async def health_check() -> Dict:
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    is_ready = model is not None and processor is not None
    
    health_info = {
        "status": "ok" if is_ready else "loading",
        "model_loaded": model is not None,
        "processor_loaded": processor is not None,
        "device": str(device),
        "cpu_threads": CPU_THREADS,
        "cpu_cores": os.cpu_count(),
        "model_id": SUPPORTED_LANGUAGES[DEFAULT_LANGUAGE]["model_id"]
    }
    
    # ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€ (psutil ìˆì„ ê²½ìš°)
    try:
        import psutil
        process = psutil.Process()
        memory_info = process.memory_info()
        health_info["memory_mb"] = round(memory_info.rss / 1024 / 1024, 1)
    except ImportError:
        pass
    
    return health_info


@app.post("/transcribe")
async def transcribe_audio(
    file: UploadFile = File(...),
    lang: str = DEFAULT_LANGUAGE
) -> JSONResponse:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (CPU ìµœì í™”)
    
    Args:
        file: ì˜¤ë””ì˜¤ íŒŒì¼ (WAV, MP3, FLAC ë“±)
        lang: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸: KR)
    
    Returns:
        JSONResponse: {"text": "ë³€í™˜ëœ í…ìŠ¤íŠ¸", "language": "KR"}
    """
    logger.info(f"ğŸ“ STT ìš”ì²­ ë°›ìŒ (íŒŒì¼: {file.filename}, ì–¸ì–´: {lang})")
    
    # ëª¨ë¸ ë¡œë“œ í™•ì¸
    if model is None or processor is None:
        raise HTTPException(
            status_code=503,
            detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
        )
    
    # ì–¸ì–´ ì§€ì› í™•ì¸
    if lang not in SUPPORTED_LANGUAGES:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. ì§€ì› ì–¸ì–´: {list(SUPPORTED_LANGUAGES.keys())}"
        )
    
    try:
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
        audio_bytes = await file.read()
        logger.info(f"ğŸ“¦ ì˜¤ë””ì˜¤ ë°ì´í„° í¬ê¸°: {len(audio_bytes)} bytes")
        
        # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        target_sr = SUPPORTED_LANGUAGES[lang]["sample_rate"]
        audio, sample_rate = librosa.load(
            io.BytesIO(audio_bytes),
            sr=target_sr,
            mono=True
        )
        logger.info(f"ğŸµ ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ (ìƒ˜í”Œë ˆì´íŠ¸: {sample_rate}Hz, ê¸¸ì´: {len(audio)} samples)")
        
        # ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì—ëŸ¬
        if len(audio) < 1600:  # 0.1ì´ˆ ë¯¸ë§Œ
            raise HTTPException(
                status_code=400,
                detail="ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 0.1ì´ˆ í•„ìš”)"
            )
        
        # â­ CPU ìµœì í™”ëœ STT ì¶”ë¡ 
        import time
        start_time = time.time()
        
        with torch.no_grad():  # â­ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            # ì…ë ¥ ì¤€ë¹„
            input_values = processor(
                audio,
                sampling_rate=sample_rate,
                return_tensors="pt"
            ).input_values
            
            # CPUë¡œ ì´ë™ (ì´ë¯¸ CPUì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ)
            input_values = input_values.to(device)
            
            # â­ ëª¨ë¸ ì¶”ë¡  (CPU)
            logits = model(input_values).logits
            
            # ë””ì½”ë”© (Greedy Decoding)
            predicted_ids = torch.argmax(logits, dim=-1)
            
            # í…ìŠ¤íŠ¸ ë³€í™˜
            transcription = processor.batch_decode(predicted_ids)[0]
        
        # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… STT ë³€í™˜ ì™„ë£Œ: '{transcription}' (ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
        
        return JSONResponse(content={
            "text": transcription,
            "language": lang,
            "model": SUPPORTED_LANGUAGES[lang]["model_id"],
            "processing_time_seconds": round(elapsed_time, 2),
            "device": "cpu",
            "cpu_threads": CPU_THREADS
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ STT ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


@app.post("/transcribe_bytes")
async def transcribe_bytes(
    audio_bytes: bytes,
    lang: str = DEFAULT_LANGUAGE,
    sample_rate: int = 16000
) -> Dict:
    """
    ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ë¥¼ ì§ì ‘ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë‚´ë¶€ API, CPU ìµœì í™”)
    
    Args:
        audio_bytes: WAV ì˜¤ë””ì˜¤ ë°”ì´íŠ¸
        lang: ì–¸ì–´ ì½”ë“œ
        sample_rate: ìƒ˜í”Œë ˆì´íŠ¸
    
    Returns:
        Dict: {"text": "ë³€í™˜ëœ í…ìŠ¤íŠ¸", "processing_time_seconds": 1.5}
    """
    logger.info(f"ğŸ“ STT ë°”ì´íŠ¸ ìš”ì²­ (í¬ê¸°: {len(audio_bytes)} bytes)")
    
    if model is None or processor is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ ë¡œë“œ ì¤‘")
    
    try:
        import time
        start_time = time.time()
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = sf.read(io.BytesIO(audio_bytes))
        
        # ëª¨ë…¸ ë³€í™˜ (ìŠ¤í…Œë ˆì˜¤ì¼ ê²½ìš°)
        if len(audio.shape) > 1:
            audio = audio.mean(axis=1)
        
        # ë¦¬ìƒ˜í”Œë§ (í•„ìš” ì‹œ)
        target_sr = SUPPORTED_LANGUAGES[lang]["sample_rate"]
        if sr != target_sr:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
        
        # â­ STT ì¶”ë¡  (CPU ìµœì í™”)
        with torch.no_grad():
            input_values = processor(
                audio,
                sampling_rate=target_sr,
                return_tensors="pt"
            ).input_values.to(device)
            
            logits = model(input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = processor.batch_decode(predicted_ids)[0]
        
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… STT ì™„ë£Œ: '{transcription}' ({elapsed_time:.2f}ì´ˆ)")
        
        return {
            "text": transcription,
            "processing_time_seconds": round(elapsed_time, 2)
        }
        
    except Exception as e:
        logger.error(f"âŒ STT ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   Wav2Vec2 STT Server (CPU Optimized)     â•‘
    â•‘   í¬íŠ¸: 8400                              â•‘
    â•‘   ëª¨ë¸: kresnik/wav2vec2-large-xlsr-koreanâ•‘
    â•‘   ë””ë°”ì´ìŠ¤: CPU                           â•‘
    â•‘   ìŠ¤ë ˆë“œ: {}                            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """.format(CPU_THREADS))
    
    print(f"ğŸ’¡ CPU ì„±ëŠ¥ ìµœì í™” íŒ:")
    print(f"   export OMP_NUM_THREADS={os.cpu_count()}  # CPU ì½”ì–´ ìˆ˜ë§Œí¼")
    print(f"   export MKL_NUM_THREADS={os.cpu_count()}")
    print()
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8400,
        log_level="info"
    )